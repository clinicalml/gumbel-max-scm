{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix - Repeated evaluation w/full state observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.067095Z",
     "start_time": "2019-04-19T16:43:04.049687Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.753547Z",
     "start_time": "2019-04-19T16:43:04.068986Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cf.counterfactual as cf\n",
    "import cf.utils as utils\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import itertools as it\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from scipy.linalg import block_diag\n",
    "\n",
    "# Sepsis Simulator code\n",
    "from sepsisSimDiabetes.State import State\n",
    "from sepsisSimDiabetes.Action import Action\n",
    "from sepsisSimDiabetes.DataGenerator import DataGenerator\n",
    "import sepsisSimDiabetes.MDP as simulator \n",
    "\n",
    "import mdptoolboxSrc.mdp as mdptools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# Avoid Type 3 fonts\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42\n",
    "\n",
    "figpath = \"./figs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.753547Z",
     "start_time": "2019-04-19T16:43:04.068986Z"
    }
   },
   "outputs": [],
   "source": [
    "fig_prefix = \"appendix-multiple-heldout-full-state-1k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.769134Z",
     "start_time": "2019-04-19T16:43:04.755230Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 1  # Note this is not the only random seed, see the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.787255Z",
     "start_time": "2019-04-19T16:43:04.770642Z"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(SEED)\n",
    "NSIMSAMPS = 1000  # Samples to draw from the simulator\n",
    "NSTEPS = 20  # Max length of each trajectory\n",
    "NCFSAMPS = 1  # Counterfactual Samples per observed sample\n",
    "DISCOUNT_Pol = 0.99 # Used for computing optimal policies\n",
    "DISCOUNT = 1 # Used for computing actual reward\n",
    "PHYS_EPSILON = 0.05 # Used for sampling using physician pol as eps greedy\n",
    "\n",
    "PROB_DIAB = 0.2\n",
    "\n",
    "# Number of iterations to get error bars\n",
    "N_REPEAT_SAMPLING = 100\n",
    "NHELDOUT = 1000 # Heldout samples for WIS\n",
    "\n",
    "# These are properties of the simulator, do not change\n",
    "n_actions = Action.NUM_ACTIONS_TOTAL\n",
    "n_components = 2\n",
    "\n",
    "# These are added as absorbing states\n",
    "# NOTE: Change to full states\n",
    "n_states_abs = State.NUM_FULL_STATES + 2\n",
    "discStateIdx = n_states_abs - 1\n",
    "deadStateIdx = n_states_abs - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:04.944514Z",
     "start_time": "2019-04-19T16:43:04.789044Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get the transition and reward matrix from file\n",
    "with open(\"./data/diab_txr_mats-replication.pkl\", \"rb\") as f:\n",
    "    mdict = pickle.load(f)\n",
    "\n",
    "tx_mat = mdict[\"tx_mat\"]\n",
    "r_mat = mdict[\"r_mat\"]\n",
    "p_mixture = np.array([1 - PROB_DIAB, PROB_DIAB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:05.064330Z",
     "start_time": "2019-04-19T16:43:04.946096Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import block_diag\n",
    "\n",
    "tx_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
    "r_mat_full = np.zeros((n_actions, State.NUM_FULL_STATES, State.NUM_FULL_STATES))\n",
    "\n",
    "for a in range(n_actions):\n",
    "    tx_mat_full[a, ...] = block_diag(tx_mat[0, a, ...], tx_mat[1, a,...])\n",
    "    r_mat_full[a, ...] = block_diag(r_mat[0, a, ...], r_mat[1, a, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:14.717995Z",
     "start_time": "2019-04-19T16:43:05.066210Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 56.9 s, sys: 420 ms, total: 57.4 s\n",
      "Wall time: 4.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "fullMDP = cf.MatrixMDP(tx_mat_full, r_mat_full)\n",
    "fullPol = fullMDP.policyIteration(discount=DISCOUNT_Pol, eval_type=1)\n",
    "\n",
    "physPolSoft = np.copy(fullPol)\n",
    "physPolSoft[physPolSoft == 1] = 1 - PHYS_EPSILON\n",
    "physPolSoft[physPolSoft == 0] = PHYS_EPSILON / (n_actions - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T16:43:14.761240Z",
     "start_time": "2019-04-19T16:43:14.740751Z"
    }
   },
   "outputs": [],
   "source": [
    "obs_reward = []\n",
    "offpol_opt_reward_WIS_hard_train = []                         \n",
    "offpol_opt_reward_WIS_hard_ho = []                         \n",
    "offpol_opt_reward_mb = []\n",
    "true_rl_reward = []\n",
    "\n",
    "# We will save the detailed samples from the first run\n",
    "saved_material = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_rl_policy(rl_policy, obs_samps, proj_lookup):\n",
    "\n",
    "    passes = True\n",
    "    # Check the observed actions for each state\n",
    "    obs_pol = np.zeros_like(rl_policy)\n",
    "    for eps_idx in range(NSIMSAMPS):\n",
    "        for time_idx in range(NSTEPS):\n",
    "            this_obs_action = int(obs_samps[eps_idx, time_idx, 1])\n",
    "            # Need to get projected state\n",
    "            if this_obs_action == -1:\n",
    "                continue\n",
    "            this_obs_state = proj_lookup[int(obs_samps[eps_idx, time_idx, 2])]\n",
    "            obs_pol[this_obs_state, this_obs_action] += 1\n",
    "\n",
    "    # Check if each RL action conforms to an observed action\n",
    "    for eps_idx in range(NSIMSAMPS):\n",
    "        for time_idx in range(NSTEPS):\n",
    "            this_full_state_unobserved = int(obs_samps[eps_idx, time_idx, 1])\n",
    "            this_obs_state = proj_lookup[this_full_state_unobserved]\n",
    "            this_obs_action = int(obs_samps[eps_idx, time_idx, 1])\n",
    "\n",
    "            if this_obs_action == -1:\n",
    "                continue\n",
    "            # This is key: In some of these trajectories, you die or get discharge.  \n",
    "            # In this case, no action is taken because the sequence has terminated, so there's nothing to compare the RL action to\n",
    "            true_death_states = r_mat[0, 0, 0, :] == -1\n",
    "            true_disch_states = r_mat[0, 0, 0, :] == 1\n",
    "            if np.logical_or(true_death_states, true_disch_states)[this_full_state_unobserved]:\n",
    "                continue\n",
    "\n",
    "            this_rl_action = rl_policy[proj_lookup[this_obs_state]].argmax()\n",
    "            if obs_pol[this_obs_state, this_rl_action] == 0:\n",
    "                print(\"Eps: {} \\t RL Action {} in State {} never observed\".format(\n",
    "                    int(time_idx / NSTEPS), this_rl_action, this_obs_state))\n",
    "                passes = False\n",
    "    return passes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the projection matrix for obs->proj states\n",
    "\n",
    "# In this case, this is an identity matrix\n",
    "n_proj_states = n_states_abs\n",
    "proj_matrix = np.eye(n_states_abs)\n",
    "\n",
    "proj_matrix = proj_matrix.astype(int)\n",
    "\n",
    "proj_lookup = proj_matrix.argmax(axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T17:12:19.338614Z",
     "start_time": "2019-04-19T16:43:14.787770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e22e4585b2ea47c6b29f9b1a67e59545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Outer Loop', style=ProgressStyle(description_width='initial')â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for it in tqdm(range(N_REPEAT_SAMPLING), desc=\"Outer Loop\"):\n",
    "    np.random.seed(it)\n",
    "    dgen = DataGenerator()\n",
    "    states, actions, lengths, rewards, diab, emp_tx_totals, emp_r_totals = dgen.simulate(\n",
    "       NSIMSAMPS, NSTEPS, policy=physPolSoft, policy_idx_type='full', \n",
    "       output_state_idx_type='full', \n",
    "       p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "    obs_samps = utils.format_dgen_samps(\n",
    "       states, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "\n",
    "    emp_tx_mat_full = np.copy(emp_tx_totals)\n",
    "    emp_r_mat_full = np.copy(emp_r_totals)\n",
    "\n",
    "    # (2) Add new aborbing states, and a new est_tx_mat with Absorbing states\n",
    "    death_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) < 0)\n",
    "    disch_states = (emp_r_mat_full.sum(axis=0).sum(axis=0) > 0)\n",
    "\n",
    "    est_tx_cts_abs = np.zeros((n_actions, n_states_abs, n_states_abs))\n",
    "    est_tx_cts_abs[:, :-2, :-2] = np.copy(emp_tx_mat_full)\n",
    "\n",
    "    death_states = np.concatenate([death_states, np.array([True, False])])\n",
    "    disch_states = np.concatenate([disch_states, np.array([False, True])])\n",
    "    assert est_tx_cts_abs[:, death_states, :].sum() == 0\n",
    "    assert est_tx_cts_abs[:, disch_states, :].sum() == 0\n",
    "\n",
    "    est_tx_cts_abs[:, death_states, deadStateIdx] = 1\n",
    "    est_tx_cts_abs[:, disch_states, discStateIdx] = 1\n",
    "\n",
    "    # (3) Project the new est_tx_cts_abs to the reduced state space\n",
    "    # PASS IN THIS CASE\n",
    "    proj_tx_cts = np.copy(est_tx_cts_abs)\n",
    "    proj_tx_mat = np.zeros_like(proj_tx_cts)\n",
    "\n",
    "    # Normalize\n",
    "    nonzero_idx = proj_tx_cts.sum(axis=-1) != 0\n",
    "    proj_tx_mat[nonzero_idx] = proj_tx_cts[nonzero_idx]\n",
    "\n",
    "    proj_tx_mat[nonzero_idx] /= proj_tx_mat[nonzero_idx].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    ############ Construct the reward matrix, which is known ##################\n",
    "    proj_r_mat = np.zeros((n_actions, n_proj_states, n_proj_states))\n",
    "    proj_r_mat[..., -2] = -1\n",
    "    proj_r_mat[..., -1] = 1\n",
    "\n",
    "    proj_r_mat[..., -2, -2] = 0 # No reward once in aborbing state\n",
    "    proj_r_mat[..., -1, -1] = 0\n",
    "\n",
    "    ############ Construct the empirical prior on the initial state ##################\n",
    "    initial_state_arr = np.copy(states[:, 0, 0])\n",
    "    initial_state_counts = np.zeros((n_states_abs,1))\n",
    "    for i in range(initial_state_arr.shape[0]):\n",
    "        initial_state_counts[initial_state_arr[i]] += 1\n",
    "\n",
    "    # Project initial state counts to new states\n",
    "    proj_state_counts = proj_matrix.T.dot(initial_state_counts).T\n",
    "    proj_p_initial_state = proj_state_counts / proj_state_counts.sum()\n",
    "\n",
    "    # Check projection is still identity\n",
    "    assert np.all(proj_state_counts.T == initial_state_counts)\n",
    "\n",
    "    # Because some SA pairs are never observed, assume they cause instant death\n",
    "    zero_sa_pairs = proj_tx_mat.sum(axis=-1) == 0\n",
    "    proj_tx_mat[zero_sa_pairs, -2] = 1  # Always insta-death if you take a never-taken action\n",
    "\n",
    "    # Construct an extra axis for the mixture component, of which there is only one\n",
    "    projMDP = cf.MatrixMDP(proj_tx_mat, proj_r_mat, \n",
    "                           p_initial_state=proj_p_initial_state)\n",
    "    try:\n",
    "        RlPol = projMDP.policyIteration(discount=DISCOUNT_Pol)\n",
    "    except:\n",
    "        assert np.allclose(proj_tx_mat.sum(axis=-1), 1)\n",
    "        RlPol = projMDP.policyIteration(discount=DISCOUNT_Pol, skip_check=True)\n",
    "\n",
    "    # Estimate the observed policy\n",
    "    obs_pol_proj = proj_tx_cts.sum(axis=-1)  # Sum over the \"to\" state\n",
    "    obs_pol_proj = obs_pol_proj.T # Switch from (a, s) to (s, a)\n",
    "    obs_states = obs_pol_proj.sum(axis=-1) > 0 # Observed \"from\" states\n",
    "\n",
    "    obs_pol_proj[obs_states] /= obs_pol_proj[obs_states].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    # Check if we always observe the RL policy in the non-absorbing states\n",
    "    prop_rl_obs = (obs_pol_proj[:-2, :][RlPol[:-2, :]==1] > 0).mean()\n",
    "    if prop_rl_obs < 1:\n",
    "        assert check_rl_policy(RlPol, obs_samps, proj_lookup), 'RL policy validation failed'\n",
    "\n",
    "    def projection_func(obs_state_idx):\n",
    "        if obs_state_idx == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return proj_lookup[obs_state_idx]\n",
    "\n",
    "    proj_f = np.vectorize(projection_func)\n",
    "    states_proj = proj_f(states)\n",
    "    assert states_proj.shape == states.shape\n",
    "\n",
    "    obs_samps_proj = utils.format_dgen_samps(\n",
    "       states_proj, actions, rewards, diab, NSTEPS, NSIMSAMPS)\n",
    "\n",
    "    # Again, projection is identity function\n",
    "    assert np.all(obs_samps_proj == obs_samps)\n",
    "\n",
    "    # Get the true RL reward as a sanity check\n",
    "    # Note that the RL policy includes actions for \"death\" and \"discharge\" absorbing states, which we ignore by taking [:-2, :]\n",
    "    NSIMSAMPS_RL = NSIMSAMPS\n",
    "    states_rl, actions_rl, lengths_rl, rewards_rl, diab_rl, _, _ = dgen.simulate(\n",
    "       NSIMSAMPS_RL, NSTEPS, policy=RlPol[:-2, :], \n",
    "       policy_idx_type='full', # Note the difference \n",
    "       p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='RL Policy Simulation')\n",
    "\n",
    "    obs_samps_rlpol = utils.format_dgen_samps(\n",
    "       states_rl, actions_rl, rewards_rl, diab_rl, NSTEPS, NSIMSAMPS_RL)\n",
    "\n",
    "    this_true_rl_reward = cf.eval_on_policy(\n",
    "        obs_samps_rlpol, discount=DISCOUNT, \n",
    "        bootstrap=False)  # Need a second axis to concat later\n",
    "\n",
    "    # This is the observed reward from the samples given\n",
    "    this_obs_reward = cf.eval_on_policy(\n",
    "        obs_samps_proj, discount=DISCOUNT, \n",
    "        bootstrap=False)\n",
    "\n",
    "   # This is the off-policy reward using WIS\n",
    "    this_offpol_opt_reward_WIS_hard_train, this_wis_samps, this_wis_ct = cf.eval_wis(\n",
    "        obs_samps_proj, discount=DISCOUNT,\n",
    "        bootstrap=False,\n",
    "        obs_policy=obs_pol_proj, new_policy=RlPol)\n",
    "\n",
    "    # Draw samples from the MDP under the new policy to get a model-based estimate of reward\n",
    "    BSampler = cf.BatchSampler(mdp=projMDP)\n",
    "    this_mb_samples_opt = BSampler.on_policy_sample(\n",
    "        policy=RlPol, n_steps=NSTEPS, n_samps=NSIMSAMPS_RL, \n",
    "        use_tqdm=False) #, tqdm_desc='Model-Based OPE') \n",
    "\n",
    "    this_offpol_opt_reward_mb = cf.eval_on_policy(\n",
    "        this_mb_samples_opt, discount=DISCOUNT,\n",
    "        bootstrap=False)\n",
    "    \n",
    "    ###################################################\n",
    "    # Construct the held-out samples, freshly each time\n",
    "    ###################################################\n",
    "    ho_dgen = DataGenerator()\n",
    "    ho_states, ho_actions, ho_lengths, ho_rewards, ho_diab, ho_emp_tx_totals, ho_emp_r_totals = ho_dgen.simulate(\n",
    "       NHELDOUT, NSTEPS, policy=physPolSoft, policy_idx_type='full', \n",
    "       output_state_idx_type='full', \n",
    "       p_diabetes=PROB_DIAB, use_tqdm=False) #True, tqdm_desc='Behaviour Policy Simulation')\n",
    "\n",
    "    ho_obs_samps = utils.format_dgen_samps(\n",
    "       ho_states, ho_actions, ho_rewards, ho_diab, NSTEPS, NHELDOUT)\n",
    "\n",
    "    ho_emp_tx_mat = np.copy(ho_emp_tx_totals)\n",
    "    ho_emp_r_mat = np.copy(ho_emp_r_totals)\n",
    "\n",
    "    ############## Construct the Transition Matrix w/proj states ##############\n",
    "    ho_proj_tx_cts = np.zeros((n_actions, n_proj_states, n_proj_states))\n",
    "    ho_proj_tx_mat = np.zeros_like(ho_proj_tx_cts)\n",
    "\n",
    "    ho_est_tx_cts = np.copy(ho_emp_tx_mat)\n",
    "    assert ho_est_tx_cts.ndim == 3\n",
    "\n",
    "    # (2) Add new aborbing states, and a new est_tx_mat with Absorbing states\n",
    "    ho_death_states = (ho_emp_r_mat.sum(axis=0).sum(axis=0) < 0)\n",
    "    ho_disch_states = (ho_emp_r_mat.sum(axis=0).sum(axis=0) > 0)\n",
    "\n",
    "    ho_est_tx_cts_abs = np.zeros((n_actions, n_states_abs, n_states_abs))\n",
    "    ho_est_tx_cts_abs[:, :-2, :-2] = np.copy(ho_est_tx_cts)\n",
    "\n",
    "    ho_death_states = np.concatenate([ho_death_states, np.array([True, False])])\n",
    "    ho_disch_states = np.concatenate([ho_disch_states, np.array([False, True])])\n",
    "    assert ho_est_tx_cts_abs[:, ho_death_states, :].sum() == 0\n",
    "    assert ho_est_tx_cts_abs[:, ho_disch_states, :].sum() == 0\n",
    "\n",
    "    ho_est_tx_cts_abs[:, ho_death_states, deadStateIdx] = 1\n",
    "    ho_est_tx_cts_abs[:, ho_disch_states, discStateIdx] = 1\n",
    "\n",
    "    # (3) Project the new est_tx_cts_abs to the reduced state space\n",
    "    for a in range(n_actions):\n",
    "        ho_proj_tx_cts[a] = proj_matrix.T.dot(ho_est_tx_cts_abs[a]).dot(proj_matrix)\n",
    "\n",
    "    # Estimate the observed policy\n",
    "    ho_obs_pol_proj = ho_proj_tx_cts.sum(axis=-1)  # Sum over the \"to\" state\n",
    "    ho_obs_pol_proj = ho_obs_pol_proj.T # Switch from (a, s) to (s, a)\n",
    "    ho_obs_states = ho_obs_pol_proj.sum(axis=-1) > 0 # Observed \"from\" states\n",
    "\n",
    "    ho_obs_pol_proj[ho_obs_states] /= ho_obs_pol_proj[ho_obs_states].sum(axis=-1, keepdims=True)\n",
    "\n",
    "    def projection_func(obs_state_idx):\n",
    "        if obs_state_idx == -1:\n",
    "            return -1\n",
    "        else:\n",
    "            return proj_lookup[obs_state_idx]\n",
    "\n",
    "    proj_f = np.vectorize(projection_func)\n",
    "    ho_states_proj = proj_f(ho_states)\n",
    "    assert ho_states_proj.shape == ho_states.shape\n",
    "\n",
    "    ho_obs_samps_proj = utils.format_dgen_samps(\n",
    "       ho_states_proj, ho_actions, ho_rewards, ho_diab, NSTEPS, NHELDOUT)\n",
    "    \n",
    "    this_offpol_opt_reward_WIS_hard_ho, this_ho_wis_samps, this_ho_wis_ct = cf.eval_wis(\n",
    "        ho_obs_samps_proj, discount=DISCOUNT,\n",
    "        bootstrap=False,\n",
    "        obs_policy=ho_obs_pol_proj, new_policy=RlPol)\n",
    "    \n",
    "    obs_reward.append(this_obs_reward)\n",
    "    offpol_opt_reward_WIS_hard_train.append(this_offpol_opt_reward_WIS_hard_train)                       \n",
    "    offpol_opt_reward_WIS_hard_ho.append(this_offpol_opt_reward_WIS_hard_ho)                       \n",
    "    offpol_opt_reward_mb.append(this_offpol_opt_reward_mb)\n",
    "    true_rl_reward.append(this_true_rl_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T17:12:19.355539Z",
     "start_time": "2019-04-19T17:12:19.340377Z"
    }
   },
   "outputs": [],
   "source": [
    "# END OF LOOP\n",
    "def conv_to_np(this_list):\n",
    "    this_arr = np.array(this_list)[:, np.newaxis]\n",
    "    this_arr = this_arr.squeeze()[:, np.newaxis]\n",
    "    return this_arr\n",
    "\n",
    "obs_reward = conv_to_np(obs_reward)\n",
    "offpol_opt_reward_WIS_hard_train = conv_to_np(offpol_opt_reward_WIS_hard_train)\n",
    "offpol_opt_reward_WIS_hard_ho = conv_to_np(offpol_opt_reward_WIS_hard_ho)\n",
    "offpol_opt_reward_mb = conv_to_np(offpol_opt_reward_mb)\n",
    "true_rl_reward = conv_to_np(true_rl_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:12:29.839468Z",
     "start_time": "2019-04-19T18:12:29.629901Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams.update({'font.size': 16})\n",
    "reward = np.concatenate([obs_reward, \n",
    "                         offpol_opt_reward_WIS_hard_train,\n",
    "                         offpol_opt_reward_WIS_hard_ho,                         \n",
    "                         offpol_opt_reward_mb,\n",
    "                         true_rl_reward,\n",
    "                         ], axis=1)\n",
    "reward_df = pd.DataFrame(reward, columns=['Obs', \n",
    "                                          'WIS (train)',\n",
    "                                          'WIS (heldout)',                                          \n",
    "                                          'MB',\n",
    "                                          'True'\n",
    "                                         ])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(data=reward_df, whis=[2.5, 97.5])\n",
    "plt.ylabel(\"Average Reward\")\n",
    "plt.savefig(\"{}/{}-ope_wis_mb_cf_true.pdf\".format(\n",
    "    figpath, fig_prefix), bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-19T18:13:58.113270Z",
     "start_time": "2019-04-19T18:13:58.086468Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS:\n",
      "\tObserved Reward:  0.3097 \t 95% Range: 0.2694 to 0.3510\n",
      "\tTrue RL Reward:\t -0.1930 \t 95% Range: -0.4051 to -0.0004\n",
      "\tWIS (train) :\t 0.5804 \t 95% Range: -0.2336 to 0.9235\n",
      "\tWIS (heldout) :\t -0.0372 \t 95% Range: -0.9379 to 0.7991\n",
      "\tMB Estimate:\t 0.5779 \t 95% Range: 0.3738 to 0.7270\n"
     ]
    }
   ],
   "source": [
    "print((\"RESULTS:\"\n",
    "       \"\\n\\tObserved Reward:  {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tTrue RL Reward:\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tWIS (train) :\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tWIS (heldout) :\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "       \"\\n\\tMB Estimate:\\t {:.4f} \\t 95% Range: {:.4f} to {:.4f}\"\n",
    "      ).format(\n",
    "    obs_reward.mean(), \n",
    "    np.quantile(obs_reward, 0.025), \n",
    "    np.quantile(obs_reward, 0.975), \n",
    "    true_rl_reward.mean(), \n",
    "    np.quantile(true_rl_reward,0.025), \n",
    "    np.quantile(true_rl_reward, 0.975),\n",
    "    offpol_opt_reward_WIS_hard_train.mean(), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_train,0.025), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_train,0.975),\n",
    "    offpol_opt_reward_WIS_hard_ho.mean(), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_ho,0.025), \n",
    "    np.quantile(offpol_opt_reward_WIS_hard_ho,0.975),\n",
    "    offpol_opt_reward_mb.mean(), \n",
    "    np.quantile(offpol_opt_reward_mb,0.025), \n",
    "    np.quantile(offpol_opt_reward_mb,0.975)\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc-showcode": false,
  "toc-showmarkdowntxt": true,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
